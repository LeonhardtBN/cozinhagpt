import os
import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# 1. Configura√ß√£o da P√°gina
st.set_page_config(page_title="CozinhaGPT", page_icon="üç≥")
st.title("üç≥ CozinheiroGPT")
st.caption("Sua assistente de cozinha pr√°tica e criativa.")

# 2. Carregar Vari√°veis de Ambiente
load_dotenv()

# Verifica√ß√£o b√°sica da API Key para evitar erros
if not os.getenv("OPENAI_API_KEY"):
    st.error("A chave OPENAI_API_KEY n√£o foi encontrada. Verifique seu arquivo .env")
    st.stop()

# 3. Configura√ß√£o da LLM e Chain (Usamos @st.cache_resource para n√£o recriar a cada intera√ß√£o)
@st.cache_resource
def get_chain():
    template = """Voc√™ √© CozinhaGPT, uma assistente de cozinha pr√°tica, criativa e gentil. 
    Seu objetivo √© transformar a lista de ingredientes que o usu√°rio informar em receitas vi√°veis ‚Äî 
    com instru√ß√µes claras, tempo estimado, n√≠vel de dificuldade e sugest√µes de substitui√ß√µes. Seja direto, objetivo e √∫til. 
    Priorize receitas que usam o m√°ximo poss√≠vel dos ingredientes fornecidos, 
    evite propor ingredientes dif√≠ceis de encontrar sem avisar, 
    e sempre ofere√ßa op√ß√µes para diferentes restri√ß√µes alimentares (vegetariano/vegano/sem gl√∫ten/lactose) quando poss√≠vel.

    Historico de conversa:
    {history}

    Entrada do Usuario:
    {input}"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", template),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}")
    ])

    llm = ChatOpenAI(temperature=0.7, model='gpt-4o-mini')
    chain = prompt | llm
    return chain

# Gerenciamento de Hist√≥rico do LangChain
# Usamos st.session_state para garantir que o hist√≥rico persista na sess√£o do Streamlit
if "store" not in st.session_state:
    st.session_state.store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in st.session_state.store:
        st.session_state.store[session_id] = ChatMessageHistory()
    return st.session_state.store[session_id]

chain = get_chain()
chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# 4. Interface de Chat
# Inicializa o hist√≥rico visual do chat (diferente da mem√≥ria do LangChain)
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ol√°! Que ingredientes voc√™ tem na geladeira hoje?"}]

# Exibe as mensagens anteriores na tela
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# Captura a entrada do usu√°rio
if prompt := st.chat_input("Digite os ingredientes (ex: ovo, tomate, queijo)..."):
    
    # Exibe a mensagem do usu√°rio
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # Gera a resposta
    with st.chat_message("assistant"):
        with st.spinner("O CozinhaGPT est√° pensando em uma receita..."):
            response = chain_with_history.invoke(
                {'input': prompt},
                config={'configurable': {'session_id': 'sessao_streamlit_usuario'}}
            )
            st.write(response.content)
    
    # Salva a resposta no hist√≥rico visual
    st.session_state.messages.append({"role": "assistant", "content": response.content})